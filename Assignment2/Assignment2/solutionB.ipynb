{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "\n",
    "START_SYMBOL = '*'\n",
    "STOP_SYMBOL = 'STOP'\n",
    "RARE_SYMBOL = '_RARE_'\n",
    "RARE_WORD_MAX_FREQ = 5\n",
    "LOG_PROB_OF_ZERO = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mylog(item, tuples):\n",
    "        if tuples.count(item) == 0:\n",
    "            return MINUS_INFINITY_SENTENCE_LOG_PROB\n",
    "        else:\n",
    "            return math.log(float(tuples.count(item)) / len(tuples),\n",
    "                            2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# Receives a list of tagged sentences and processes each sentence to generate a list of words and a list of tags.\n",
    "# Each sentence is a string of space separated \"WORD/TAG\" tokens, with a newline character in the end.\n",
    "# Remember to include start and stop symbols in yout returned lists, as defined by the constants START_SYMBOL and STOP_SYMBOL.\n",
    "# brown_words (the list of words) should be a list where every element is a list of the tags of a particular sentence.\n",
    "# brown_tags (the list of tags) should be a list where every element is a list of the tags of a particular sentence.\n",
    "def split_wordtags(brown_train):\n",
    "    brown_words = []\n",
    "    brown_tags = []\n",
    "    for s in brown_train:\n",
    "        brown_words.append(START_SYMBOL)\n",
    "        brown_tags.append(START_SYMBOL)\n",
    "        for ss in s.split():\n",
    "            if ss != '\\n':\n",
    "                brown_tags.append(ss.split('/')[-1])\n",
    "                brown_words.append(ss.replace('/'+ss.split('/')[-1],''))\n",
    "            else:\n",
    "                brown_words.append(STOP_SYMBOL)\n",
    "                brown_tags.append(STOP_SYMBOL)\n",
    "    return brown_words, brown_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# This function takes tags from the training data and calculates tag trigram probabilities.\n",
    "# It returns a python dictionary where the keys are tuples that represent the tag trigram, and the values are the log probability of that trigram\n",
    "def calc_trigrams(brown_tags):\n",
    "    q_values = {}\n",
    "    trigram_tuples = list(nltk.trigrams(brown_tags))    \n",
    "    q_values = {item: mylog(item, trigram_tuples)\n",
    "                 for item in set(trigram_tuples)}\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes output from calc_trigrams() and outputs it in the proper format\n",
    "def q2_output(q_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    trigrams = q_values.keys()\n",
    "    trigrams.sort()  \n",
    "    for trigram in trigrams:\n",
    "        output = \" \".join(['TRIGRAM', trigram[0], trigram[1], trigram[2], str(q_values[trigram])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# Takes the words from the training data and returns a set of all of the words that occur more than 5 times (use RARE_WORD_MAX_FREQ)\n",
    "# brown_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# Note: words that appear exactly 5 times should be considered rare!\n",
    "def calc_known(brown_words):\n",
    "    wordsset = set(brown_words)\n",
    "    wordslist = []\n",
    "    for s in wordsset:\n",
    "        if brown_words.count(s) > RARE_WORD_MAX_FREQ:\n",
    "            wordslist.append(s)\n",
    "    known_words = set(wordslist)\n",
    "    return known_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# Takes the words from the training data and a set of words that should not be replaced for '_RARE_'\n",
    "# Returns the equivalent to brown_words but replacing the unknown words by '_RARE_' (use RARE_SYMBOL constant)\n",
    "def replace_rare(brown_words, known_words):\n",
    "    brown_words_rare = []\n",
    "    for s in brown_words:\n",
    "        if s not in known_words:\n",
    "            brown_words_rare.append(RARE_SYMBOL)\n",
    "        else:\n",
    "            brown_words_rare.append(s)\n",
    "    return brown_words_rare\n",
    "\n",
    "# This function takes the ouput from replace_rare and outputs it to a file\n",
    "def q3_output(rare, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in rare:\n",
    "        outfile.write(' '.join(sentence[2:-1]) + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# Calculates emission probabilities and creates a set of all possible tags\n",
    "# The first return value is a python dictionary where each key is a tuple in which the first element is a word\n",
    "# and the second is a tag, and the value is the log probability of the emission of the word given the tag\n",
    "# The second return value is a set of all possible tags for this data set\n",
    "def calc_emission(brown_words_rare, brown_tags):\n",
    "    e_values = {}\n",
    "    word_tag_tuples = []\n",
    "\n",
    "    for word,tag in zip(brown_words_rare,brown_tags):\n",
    "        word_tag_tuples.append((word,tag))\n",
    "\n",
    "    for tuples in set((word_tag_tuples)):\n",
    "        e_values[tuples] = mylog(tuples,word_tag_tuples)\n",
    "\n",
    "    taglist = set(brown_tags)\n",
    "    return e_values, taglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function takes the output from calc_emissions() and outputs it\n",
    "def q4_output(e_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    emissions = e_values.keys()\n",
    "    emissions.sort()  \n",
    "    for item in emissions:\n",
    "        output = \" \".join([item[0], item[1], str(e_values[item])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# This function takes data to tag (brown_dev_words), a set of all possible tags (taglist), a set of all known words (known_words),\n",
    "# trigram probabilities (q_values) and emission probabilities (e_values) and outputs a list where every element is a tagged sentence \n",
    "# (in the WORD/TAG format, separated by spaces and with a newline in the end, just like our input tagged data)\n",
    "# brown_dev_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# taglist is a set of all possible tags\n",
    "# known_words is a set of all known words\n",
    "# q_values is from the return of calc_trigrams()\n",
    "# e_values is from the return of calc_emissions()\n",
    "# The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a \n",
    "# terminal newline, not a list of tokens. Remember also that the output should not contain the \"_RARE_\" symbol, but rather the\n",
    "# original words of the sentence!\n",
    "def viterbi(brown_dev_words, taglist, known_words, q_values, e_values):\n",
    "    tagged = []\n",
    "    return tagged\n",
    "\n",
    "# This function takes the output of viterbi() and outputs it to file\n",
    "def q5_output(tagged, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in tagged:\n",
    "        outfile.write(sentence)\n",
    "    outfile.close()\n",
    "\n",
    "# TODO: IMPLEMENT THIS FUNCTION\n",
    "# This function uses nltk to create the taggers described in question 6\n",
    "# brown_words and brown_tags is the data to be used in training\n",
    "# brown_dev_words is the data that should be tagged\n",
    "# The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a \n",
    "# terminal newline, not a list of tokens. \n",
    "def nltk_tagger(brown_words, brown_tags, brown_dev_words):\n",
    "    # Hint: use the following line to format data to what NLTK expects for training\n",
    "    training = [ zip(brown_words[i],brown_tags[i]) for i in xrange(len(brown_words)) ]\n",
    "\n",
    "    # IMPLEMENT THE REST OF THE FUNCTION HERE\n",
    "    tagged = []\n",
    "    return tagged\n",
    "\n",
    "# This function takes the output of nltk_tagger() and outputs it to file\n",
    "def q6_output(tagged, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in tagged:\n",
    "        outfile.write(sentence)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "OUTPUT_PATH = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# start timer\n",
    "time.clock()\n",
    "\n",
    "# open Brown training data\n",
    "infile = open(DATA_PATH + \"Brown_tagged_train.txt\", \"r\")\n",
    "brown_train = infile.readlines()[:100]\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At/ADP that/DET time/NOUN highway/NOUN engineers/NOUN traveled/VERB rough/ADJ and/CONJ dirty/ADJ roads/NOUN to/PRT accomplish/VERB their/DET duties/NOUN ./. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using/VERB privately-owned/ADJ vehicles/NOUN was/VERB a/DET personal/ADJ hardship/NOUN for/ADP such/ADJ employees/NOUN ,/. and/CONJ the/DET matter/NOUN of/ADP providing/VERB state/NOUN transportation/NOUN was/VERB felt/VERB perfectly/ADV justifiable/ADJ ./. \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split words and tags, and add start and stop symbols (question 1)\n",
    "brown_words, brown_tags = split_wordtags(brown_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = ' '.join(brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate tag trigram probabilities (question 2)\n",
    "q_values = calc_trigrams(brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 2 output\n",
    "q2_output(q_values, OUTPUT_PATH + 'B2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsset = set(brown_words)\n",
    "wordslist = []\n",
    "for s in wordsset:\n",
    "    if brown_words.count(s) > RARE_WORD_MAX_FREQ:\n",
    "        wordslist.append(s)\n",
    "known_words = set(wordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate list of words with count > 5 (question 3)\n",
    "known_words = calc_known(brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = brown_words[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s not in known_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "brown_words_rare = replace_rare(brown_words, known_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 3 output\n",
    "q3_output(brown_words_rare, OUTPUT_PATH + \"B3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(brown_words_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate emission probabilities (question 4)\n",
    "e_values, taglist = calc_emission(brown_words_rare, brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 4 output\n",
    "q4_output(e_values, OUTPUT_PATH + \"B4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete unneceessary data\n",
    "del brown_train\n",
    "del brown_words_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open Brown development data (question 5)\n",
    "infile = open(DATA_PATH + \"Brown_dev.txt\", \"r\")\n",
    "brown_dev = infile.readlines()\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format Brown development data here\n",
    "brown_dev_words = []\n",
    "for sentence in brown_dev:\n",
    "    brown_dev_words.append(sentence.split(\" \")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# do viterbi on brown_dev_words (question 5)\n",
    "viterbi_tagged = viterbi(brown_dev_words, taglist, known_words, q_values, e_values)\n",
    "\n",
    "# question 5 output\n",
    "q5_output(viterbi_tagged, OUTPUT_PATH + 'B5.txt')\n",
    "\n",
    "# do nltk tagging here\n",
    "nltk_tagged = nltk_tagger(brown_words, brown_tags, brown_dev_words)\n",
    "\n",
    "# question 6 output\n",
    "q6_output(nltk_tagged, OUTPUT_PATH + 'B6.txt')\n",
    "\n",
    "# print total time to run Part B\n",
    "print \"Part B time: \" + str(time.clock()) + ' sec'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
